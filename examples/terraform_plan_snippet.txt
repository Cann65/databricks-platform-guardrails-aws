Terraform Plan Output (Excerpt)
================================

This is a sample output from running `make plan` in the dev environment.

Note: Actual output requires configured AWS credentials and Databricks workspace.
The plan shows what resources would be created.

$ cd terraform/envs/dev
$ terraform plan

Terraform used the selected providers to generate the following execution plan.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # module.aws_baseline.aws_iam_instance_profile.databricks will be created
  + resource "aws_iam_instance_profile" "databricks" {
      + arn         = (known after apply)
      + create_date = (known after apply)
      + id          = (known after apply)
      + name        = "databricks-dev-instance-profile"
      + path        = "/"
      + role        = "databricks-dev-instance-profile"
      + tags        = {
          + "CostCenter"  = "data-platform"
          + "Environment" = "dev"
          + "Owner"       = "platform-team"
        }
      + tags_all    = {
          + "CostCenter"  = "data-platform"
          + "Environment" = "dev"
          + "ManagedBy"   = "terraform"
          + "Owner"       = "platform-team"
          + "Project"     = "databricks-platform-guardrails"
        }
      + unique_id   = (known after apply)
    }

  # module.aws_baseline.aws_s3_bucket.databricks_root_storage will be created
  + resource "aws_s3_bucket" "databricks_root_storage" {
      + acceleration_status         = (known after apply)
      + acl                         = (known after apply)
      + arn                         = (known after apply)
      + bucket                      = "databricks-dev-storage-12345"
      + bucket_domain_name          = (known after apply)
      + bucket_prefix               = (known after apply)
      + bucket_regional_domain_name = (known after apply)
      + force_destroy               = true
      + hosted_zone_id              = (known after apply)
      + id                          = (known after apply)
      + object_lock_enabled         = (known after apply)
      + policy                      = (known after apply)
      + region                      = (known after apply)
      + request_payer               = (known after apply)
      + tags                        = {
          + "CostCenter"   = "data-platform"
          + "Description"  = "Root storage bucket for Databricks workspace"
          + "Environment"  = "dev"
          + "Name"         = "databricks-dev-storage-12345"
          + "Owner"        = "platform-team"
        }
      + tags_all                    = {
          + "CostCenter"   = "data-platform"
          + "Description"  = "Root storage bucket for Databricks workspace"
          + "Environment"  = "dev"
          + "ManagedBy"    = "terraform"
          + "Name"         = "databricks-dev-storage-12345"
          + "Owner"        = "platform-team"
          + "Project"      = "databricks-platform-guardrails"
        }
      + website_domain              = (known after apply)
      + website_endpoint            = (known after apply)
    }

  # module.aws_baseline.aws_s3_bucket_policy.databricks_root_storage_tls will be created
  + resource "aws_s3_bucket_policy" "databricks_root_storage_tls" {
      + bucket = (known after apply)
      + id     = (known after apply)
      + policy = jsonencode(
            {
              + Statement = [
                  + {
                      + Action    = "s3:*"
                      + Condition = {
                          + Bool = {
                              + "aws:SecureTransport" = "false"
                            }
                        }
                      + Effect    = "Deny"
                      + Principal = "*"
                      + Resource  = [
                          + (known after apply),
                          + (known after apply),
                        ]
                      + Sid       = "DenyInsecureTransport"
                    },
                ]
              + Version   = "2012-10-17"
            }
        )
    }

  # module.aws_baseline.aws_s3_bucket_server_side_encryption_configuration.databricks_root_storage will be created
  + resource "aws_s3_bucket_server_side_encryption_configuration" "databricks_root_storage" {
      + bucket = (known after apply)
      + id     = (known after apply)

      + rule {
          + apply_server_side_encryption_by_default {
              + sse_algorithm = "AES256"
            }
        }
    }

  # module.databricks_guardrails.databricks_cluster_policy.guardrails_default will be created
  + resource "databricks_cluster_policy" "guardrails_default" {
      + definition    = jsonencode(
            {
              + autoscale.max_workers                = {
                  + maxValue = 8
                  + minValue = 1
                  + type     = "range"
                }
              + autoscale.min_workers                = {
                  + maxValue = 8
                  + minValue = 1
                  + type     = "range"
                }
              + autotermination_minutes              = {
                  + defaultValue = 15
                  + maxValue     = 15
                  + minValue     = 10
                  + type         = "range"
                }
              + "custom_tags.cost_center"            = {
                  + defaultValue = "data-platform"
                  + type         = "unlimited"
                }
              + "custom_tags.env"                    = {
                  + type  = "fixed"
                  + value = "dev"
                }
              + "custom_tags.owner"                  = {
                  + type  = "fixed"
                  + value = "platform-team"
                }
              + driver_node_type_id                  = {
                  + defaultValue = "i3.xlarge"
                  + type         = "allowlist"
                  + values       = [
                      + "i3.xlarge",
                      + "i3.2xlarge",
                      + "m5d.large",
                      + "m5d.xlarge",
                    ]
                }
              + node_type_id                         = {
                  + defaultValue = "i3.xlarge"
                  + type         = "allowlist"
                  + values       = [
                      + "i3.xlarge",
                      + "i3.2xlarge",
                      + "m5d.large",
                      + "m5d.xlarge",
                    ]
                }
              + num_workers                          = {
                  + maxValue = 8
                  + minValue = 1
                  + type     = "range"
                }
            }
        )
      + description   = "Platform guardrails policy: enforces cost controls, auto-termination, and required tagging"
      + id            = (known after apply)
      + name          = "guardrails-default"
      + policy_family_definition_overrides = (known after apply)
      + policy_family_id                   = (known after apply)
    }

  # module.databricks_guardrails.databricks_secret_scope.platform will be created
  + resource "databricks_secret_scope" "platform" {
      + backend_type = (known after apply)
      + id           = (known after apply)
      + name         = "platform"
    }

Plan: 6 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + cluster_policy_id   = (known after apply)
  + cluster_policy_name = "guardrails-default"
  + instance_profile_arn = (known after apply)
  + s3_bucket_name      = "databricks-dev-storage-12345"
  + secret_scope_name   = "platform"

────────────────────────────────────────────────────────────────────────────────

Note: This plan was created without Databricks credentials. In a real deployment:
1. Set DATABRICKS_HOST and DATABRICKS_TOKEN environment variables
2. Run terraform init to download providers
3. Run terraform plan to see full output
4. Review and run terraform apply to create resources
5. Run make audit to validate compliance

Security Notes:
- S3 bucket has force_destroy=true in dev (safe for testing)
- TLS-only policy enforced on S3 bucket
- Auto-termination enforced at 15 minutes maximum
- Cluster size limited to 8 workers maximum
- Required tags enforced: owner, cost_center, env
