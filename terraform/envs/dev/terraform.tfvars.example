# Example terraform.tfvars file
# Copy to terraform.tfvars and customize

# AWS Configuration
aws_region = "us-east-1"

# S3 bucket must be globally unique
s3_bucket_name = "databricks-dev-storage-REPLACE_WITH_UNIQUE_SUFFIX"
s3_force_destroy = true

# IAM Instance Profile
instance_profile_role_name = "databricks-dev-instance-profile"

# Tagging
owner_tag       = "platform-team"
cost_center_tag = "data-platform"

# Cluster Guardrails
max_workers                 = 8
max_autotermination_minutes = 15
default_node_type           = "i3.xlarge"

# Cost control: restrict to smaller node types
allowed_node_types = [
  "i3.xlarge",
  "i3.2xlarge",
  "m5d.large",
  "m5d.xlarge"
]

# Demo job (disabled by default to avoid costs)
enable_demo_job = false

# Spark runtime for jobs
spark_version = "13.3.x-scala2.12"

# Workload job (optional)
enable_workload_job             = false
workload_job_name               = "guardrails-demo-pipeline"
# Update notebook_base_path after importing repo into Databricks Repos or /Shared
notebook_base_path              = "/Repos/<user>/databricks-platform-guardrails-aws/workload/notebooks"
output_base_path                = "dbfs:/tmp/guardrails_demo"
workload_job_workers            = 2
workload_autotermination_minutes = 15
